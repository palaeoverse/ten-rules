---
title: "10 Rules vignette"
author: "The Palaeoverse Development Team"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{10 Rules vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\vignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, include = TRUE, eval = TRUE, dpi = 200,
                      out.width = "100%", fig.width = 7.2, fig.height = 4)
```

# Introduction

Here we present a full example workflow that complies with the 10 Rules we propose in the manuscript.

## Load packages

```{r load_packages, message = FALSE}
#install.packages(c("dplyr", "palaeoverse", "readr"))
library(dplyr)
library(palaeoverse)
library(readr)
```

## Rule 1: Know your data and question

In this example, we will explore, clean and interrogate the fossil record of crocodiles. As ectotherms, crocodiles are highly reliant on the environment in which they live in order to maintain a functional internal body temperature. Because of this, their spatial distribution is constrained to warm climates, so their fossil record is commonly used as an indicator of palaeoclimate. Here we will investigate the palaeodiversity of crocodiles, with the specific goal of reconstructing their biogeographic patterns throughout the Paleogene. The data will be sourced from the [Paleobiology Database](https://paleobiodb.org).

## Rule 2: Keep raw data raw

[Bethany]
# Would it be better to download directly from the API and show the first step as saving the file?

We opted to download our data by completing the 'Download' form on the Paleobiology Database website. Practically this uses the options entered into the form to create a call to the database's API (application programming interface), and then enacts this call. More information about the API service can be found [here](https://paleobiodb.org/data1.2/).

For our dataset, we pulled all occurrences associated with the taxon "Crocodylia", dated to the "Paleogene", in the form of a ".csv" file. All other settings were left at default. We will see later in the vignette what this means in terms of what is contained within the dataset.

For reproducibility, we want to make sure that we have a copy of the full dataset as initially downloaded - this is the "raw" data. Automatically, when we use the online form, there is an output option checked which is labelled 'Include metadata at the beginning of the output'. This ensures that the raw data file includes a metadata 'header', which contains useful information linked to the dataset.

We can load and view the metadata to see what it includes.

```{r read_metadata}
# Load data file
metadata <- read.csv("../data/Paleogene_crocs.csv")
# Trim to metadata
metadata <- metadata[1:23,]
# Print
metadata
```

The metadata are strangely formatted here, but we can see that they include information about the data license (CC0), the API call used (under the label 'Data URL'), the date and time at which the data were accessed, and the total number of records contained within the dataset.

These metadata elements are all important information to retain alongside our data, allowing others to better understand what the dataset contains, and when and how it was downloaded. The Paleobiology Database is fully dynamic, not only in that new data is continually being added, but also any record can be changed retrospectively by an Editor. It cannot be assumed that the 'present' state of any data point was the same in the past. So, for example, if someone wanted to see how the data associated with this API call had changed in the time elapsed since our download, they could do this, and directly in R if desired:

```{r API_call, eval = FALSE}
# Pull API URL
metadata[5,2]
# Use API call (this is not enacted)
new_data <- read.csv(metadata[5,2], header = TRUE)
```

While the metadata is important to keep in the raw file, for the purposes of analysis, we want to be able to just read in the data beneath it. We can do this using the `skip` parameter in `read.csv`, which tells R to ignore the given number of rows at the top of the file.

```{r read_raw, message = FALSE}
fossils <- read_csv("../data/Paleogene_crocs.csv", skip = 19)
```

And now we are ready to commence our data exploration and cleaning.

## Rule 3: Document your workflow

[All]

Documenting your workflow is essential for ensuring that others can understand and replicate all steps. Using programming languages such as R or Python makes this easy, as the code used can be provided alongside the data. Our vignette gives an example of how `markdown` can be a particularly useful tool for integrating code with prose, ensuring that everything is fully explained, and providing plots integrated within the text when exported.

Our tips for good documentation in R include:
- Following general guidelines for good coding, such as ample use of spacing to help human readability
- Having a clear, overarching code structure (such as subheadings, and an order which corresponds to the flow of the research methods/results)
- Using logical yet short variable names
- Including comments throughout which explain both *what* and *why*, 

## Rule 4: Explore your data

[Will]

The first thing we want to do with our data is *explore* it - that is, generate summary statistics and plots to help us understand the data and its various characteristics.

```{r load_packages2, message = FALSE}
library(ggplot2)
library(deeptime)
library(rnaturalearth)
library(rnaturalearthdata)
```

First we'll plot where the crocodile fossils have been found across the globe.

```{r map}
world <- ne_countries(scale = "medium", returnclass = "sf")
ggplot(fossils) +
  geom_sf(data = world) +
  geom_point(aes(x = lng, y = lat)) +
  labs(x = "Longitude",
       y = "Latitude") +
  coord_sf()
```

We have a large density of crocodile occurrences in Europe and the western interior of the United States along with a smattering of occurrences across the other continents. The crocodile occurrences in Antarctica seem particularly suspicious. Let's investigate further. We'll plot the latitude of the occurrences through time.

```{r lat_time}
fossils <- fossils %>%
  mutate(mid_age = (min_ma + max_ma) / 2)

ggplot(fossils, aes(x = mid_age, y = lat)) +
  geom_point() +
  labs(x = "Age (Ma)",
       y = "Latitude") +
  scale_x_reverse() +
  coord_geo(dat = "stages", expand = TRUE)
```

When we look at latitude through time, these occurrences look even more suspicious. But, wait, we should actually be looking at **paleo**latitude instead. Let's plot that against time.

```{r paleolat_time}
fossils <- fossils %>%
  mutate(mid_age = (min_ma + max_ma) / 2)

ggplot(fossils, aes(x = mid_age, y = paleolat)) +
  geom_point() +
  labs(x = "Age (Ma)",
       y = "Paleolatitude") +
  scale_x_reverse() +
  coord_geo(dat = "stages", expand = TRUE)
```

Hmm...when we look at paleolatitude the occurrences are even **more** south. Time to really check out these occurrences...

Well, upon further investigation in the Paleobiology Database, the collections that these occurrences come from ([31173](https://paleobiodb.org/classic/basicCollectionSearch?collection_no=31173), [120887](https://paleobiodb.org/classic/basicCollectionSearch?collection_no=120887), and [43030](https://paleobiodb.org/classic/basicCollectionSearch?collection_no=43030)).
 all appear to be fairly legitimate. However, all three occurrences still appear to be outliers, especially in the late Eocene [when temperatures were dropping](https://doi.org/10.1038/s41586-018-0272-2). Since all three occurrences are listed as Crocodylia indet., it may make sense to remove them from further analyses.

Let's investigate if there are any other anomalies or outliers in our data. We'll bin the occurrences by stage to look for stage-level outliers.

```{r lat_time_binned}
bins <- time_bins(scale = "international ages")
fossils <- bin_time(occdf = fossils, bins = bins,
                    min_ma = "min_ma", max_ma = "max_ma", method = "majority") %>%
  left_join(bins, by = c("bin_assignment" = "bin"))

ggplot(fossils, aes(x = mid_ma, y = paleolat, fill = interval_name)) +
  geom_boxplot(scale = "width", show.legend = FALSE) +
  labs(x = "Age (Ma)",
       y = "Paleolatitude") +
  scale_x_reverse() +
  scale_fill_geo("stages") +
  coord_geo(dat = "stages", expand = TRUE)
```

Hmm, the Ypresian is looking pretty suspicious. Let's plot the Ypresian occurrences on a paleogeographic map to investigate further.

```{r map_ypresian}
library(rgplates)
fossils_y <- fossils %>%
  filter(interval_name == "Ypresian")
world_y <- reconstruct("coastlines", age = 51.9)

ggplot(fossils_y) +
  geom_sf(data = world_y) +
  geom_point(aes(x = paleolng, y = paleolat)) +
  labs(x = "Paleolongitude",
       y = "Paleolatitude") +
  coord_sf()
```

Aha! There is a concentrated cluster of occurrences in the western interior of North America. Without this strong geographic bias, all of the occurrences in the Ypresian appear to be normal, especially with [elevated temperatures during this time](https://doi.org/10.1038/s41586-018-0272-2).

```{r lat_ypresian}
library(rgplates)
fossils_y <- fossils_y %>%
  filter(paleolat < 45)

ggplot(fossils_y) +
  geom_histogram(aes(x = paleolat)) +
  labs(x = "Paleolatitude")
```

## Rule 5: Handle incomplete data records

[?]

By default, when we read data tables into R, it recognises empty cells and takes some course of action to manage them. When we use base R functions, such as `read.csv()`, empty cells are given an `NA` value ('not applicable') only when the column is considered to contain numerical data. When we use `Tidyverse` functions, such as `readr::read_csv()`, *all* empty cells are given `NA` values. This is important to bear in mind when we want to find those missing values: here, we have done the latter, so all empty cells are `NA`.

The extent of incompleteness of the different columns in our dataset is highly variable. For example, the number of `NA` values for the `collection_no` is 0.

```{r count_NA_collections}
# Count the number of collection number values for which `is.na()` is TRUE
sum(is.na(fossils$collection_no))
```

This is because it is impossible to add an occurrence to the PBDB without putting it in a collection, which must in turn have an identification number.

However, what about `genus`?

```{r count_NA_genera}
# Count the number of genus IDs for which `is.na()` is TRUE
sum(is.na(fossils$genus))
```

The value here is 382. This represents the number of occurrences in our dataset which cannot be placed in a genus, i.e. their taxonomic identification is to a coarser taxonomic level.

What can, or should, we do about this? It depends heavily on the intersection between the different variables in the dataset, and our research question. For example, in some instances it might be appropriate to ignore certain `NA` values (for example, `NA` values for `genus` might not matter if we want to see the spatial distribution of all crocodylian fossils, regardless of how coarsely identified they are). Alternatively, it might be approriate to filter these values out (for example, if we wanted to calculate the relative abundance of different crocodylian genera, an `NA` in this data column would render the occurrence irrelevant for this specific analysis).

A further option applicable in some cases would be to fill in our `NA`s. We may be able to interpolate values from the rest of our data, or use additional data sources.

## Rule 6: Identify outliers

[Bethany]

## Rule 7: Identify inconsistencies

[Erin]

## Rule 8: Identify duplicates

[Ale]

## Rule 9: Report your data cleaning efforts

[All]

"Here is what we would write in the paper"

## Rule 10: Deposit your data workflow and recording

[Will]

# Summary
