---
title: "10 Rules vignette"
author: "The Palaeoverse Development Team"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{10 Rules vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\vignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, include = TRUE, eval = TRUE, dpi = 200,
                      out.width = "100%", fig.width = 7.2, fig.height = 4)
```

# Introduction

Here we present a full example workflow that complies with the 10 Rules we propose in the manuscript.

## Load packages

```{r load_packages, message = FALSE}
#install.packages(c("dplyr", "palaeoverse", "readr", "ggplot2", "deeptime", "rnaturalearth", "rnaturalearthdata", "rgplates"))
library(dplyr)
library(palaeoverse)
library(readr)
library(ggplot2)
library(deeptime)
library(rnaturalearth)
library(rnaturalearthdata)
library(rgplates)
library(fossilbrush)
library(CoordinateCleaner)
```

## Rule 1: Know your data and question

In this example, we will explore, clean and interrogate the fossil record of crocodiles. As ectotherms, crocodiles are highly reliant on the environment in which they live in order to maintain a functional internal body temperature. Because of this, their spatial distribution is constrained to warm climates, so their fossil record is commonly used as an indicator of palaeoclimate. Here we will investigate the palaeodiversity of crocodiles, with the specific goal of reconstructing their biogeographic patterns throughout the Paleogene. The data will be sourced from the [Paleobiology Database](https://paleobiodb.org).

## Rule 2: Keep raw data raw

[Bethany]
# Would it be better to download directly from the API and show the first step as saving the file?

We opted to download our data by completing the 'Download' form on the Paleobiology Database website. Practically this uses the options entered into the form to create a call to the database's API (application programming interface), and then enacts this call. More information about the API service can be found [here](https://paleobiodb.org/data1.2/).

For our dataset, we pulled all occurrences associated with the taxon "Crocodylia", dated to the "Paleogene", in the form of a ".csv" file. All other settings were left at default. We will see later in the vignette what this means in terms of what is contained within the dataset.

For reproducibility, we want to make sure that we have a copy of the full dataset as initially downloaded - this is the "raw" data. Automatically, when we use the online form, there is an output option checked which is labelled 'Include metadata at the beginning of the output'. This ensures that the raw data file includes a metadata 'header', which contains useful information linked to the dataset.

We can load and view the metadata to see what it includes.

```{r read_metadata}
# Load data file
metadata <- read.csv("../data/Paleogene_crocs.csv")
# Trim to metadata
metadata <- metadata[1:23,]
# Print
metadata
```

The metadata are strangely formatted here, but we can see that they include information about the data license (CC0), the API call used (under the label 'Data URL'), the date and time at which the data were accessed, and the total number of records contained within the dataset.

These metadata elements are all important information to retain alongside our data, allowing others to better understand what the dataset contains, and when and how it was downloaded. The Paleobiology Database is fully dynamic, not only in that new data is continually being added, but also any record can be changed retrospectively by an Editor. It cannot be assumed that the 'present' state of any data point was the same in the past. So, for example, if someone wanted to see how the data associated with this API call had changed in the time elapsed since our download, they could do this, and directly in R if desired:

```{r API_call, eval = FALSE}
# Pull API URL
metadata[5,2]
# Use API call (this is not enacted)
new_data <- read.csv(metadata[5,2], header = TRUE)
```

While the metadata is important to keep in the raw file, for the purposes of analysis, we want to be able to just read in the data beneath it. We can do this using the `skip` parameter in `read.csv`, which tells R to ignore the given number of rows at the top of the file.

```{r read_raw, message = FALSE}
fossils <- read_csv("../data/Paleogene_crocs.csv", skip = 19)
```

And now we are ready to commence our data exploration and cleaning.

## Rule 3: Document your workflow

[All]

Documenting your workflow is essential for ensuring that others can understand and replicate all steps. Using programming languages such as R or Python makes this easy, as the code used can be provided alongside the data. Our vignette gives an example of how `markdown` can be a particularly useful tool for integrating code with prose, ensuring that everything is fully explained, and providing plots integrated within the text when exported.

Our tips for good documentation in R include:
- Following general guidelines for good coding, such as ample use of spacing to help human readability
- Having a clear, overarching code structure (such as subheadings, and an order which corresponds to the flow of the research methods/results)
- Using logical yet short variable names
- Including comments throughout which explain both *what* and *why*, 

## Rule 4: Explore your data

[Will]

The first thing we want to do with our data is *explore* it - that is, generate summary statistics and plots to help us understand the data and its various characteristics.

For example, we can look at the distribution of identification levels for our fossils.

```{r ID_distribution}
# Count the frequency of taxonomic ranks, and add a column which calculates the proportion
count(fossils, accepted_rank) %>% mutate(fraction = (n/nrow(fossils) * 100))
```

We can see that of our 886 occurrences, 250 (28%) are identified to species level. A further 254 (29%) are identified to genus level. The remaining fossils are more coarsely identified, including 365 (41%) which are identified to the mysterious level of "unranked clade".

Next, let's look at the distribution of fossils across localities. In the PBDB, fossils are placed within collections, each of which can roughly be considered a separate locality (they can also represent different sampling horizons at the same locality). First, we can count the number of unique `collection_no` values to find out how many unique collections are in the dataset.

```{r unique_colls}
# What is the length of a vector of unique collection numbers?
length(unique(fossils$collection_no))
```

Our dataset contains 720 unique localities. We can also create a plot showing us the distribution of occurrences across these localities.

```{r abundance_distribution}
# Create a plot showing the number of fossils in each collection
ggplot(fossils, aes(x = as.factor(collection_no))) +
  geom_histogram(stat = "count") +
  labs(x = "Collection number",
       y = "Abundance")
```

We can see that the collection containing the most occurrences has 7, while the vast majority only contain a single occurrence.

What about the countries in which these fossils were found? We can investigate this using the "cc", or "country code" column.

```{r countries}
# List unique country codes, and count them
unique(fossils$cc...36)
length(unique(fossils$cc...36))
```

Here we can see that Paleogene crocodiles have been found in 46 different countries. Let's sort those values alphabetically to help us find specific countries.

```{r countries_2}
# List and sort unique country codes, and count them
sort(unique(fossils$cc...36))
length(sort(unique(fossils$cc...36)))
```

Something weird has happened here: we can see that once the countries have been sorted, one of them has disappeared. Why? We will come back to this during our next rule.

## Rule 5: Handle incomplete data records

[?]

By default, when we read data tables into R, it recognises empty cells and takes some course of action to manage them. When we use base R functions, such as `read.csv()`, empty cells are given an `NA` value ('not applicable') only when the column is considered to contain numerical data. When we use `Tidyverse` functions, such as `readr::read_csv()`, *all* empty cells are given `NA` values. This is important to bear in mind when we want to find those missing values: here, we have done the latter, so all empty cells are `NA`.

The extent of incompleteness of the different columns in our dataset is highly variable. For example, the number of `NA` values for the `collection_no` is 0.

```{r count_NA_collections}
# Count the number of collection number values for which `is.na()` is TRUE
sum(is.na(fossils$collection_no))
```

This is because it is impossible to add an occurrence to the PBDB without putting it in a collection, which must in turn have an identification number.

However, what about `genus`?

```{r count_NA_genera}
# Count the number of genus IDs for which `is.na()` is TRUE
sum(is.na(fossils$genus))
```

The value here is 382. This corresponds to the number of occurrences in our dataset which cannot be placed in a genus, i.e. their taxonomic identification is to a coarser taxonomic level.

What can, or should, we do about this? It depends heavily on the intersection between the different variables in the dataset, and our research question. For example, in some instances it might be appropriate to ignore certain `NA` values (for example, `NA` values for `genus` might not matter if we want to see the spatial distribution of all crocodylian fossils, regardless of how coarsely identified they are).

Alternatively, it might be approriate to filter these values out. For our crocodile example, we are interested in biogeography, so the latitude and longitude, and palaeolatitude and palaeolongitude, are pretty important. Do we have missing values there?

```{r missing_geo}
# Count the number of occurrences for which `is.na()` is TRUE in latitude, longitude, palaeolatitude, and palaeolongitude
sum(is.na(fossils$lat))
sum(is.na(fossils$lng))
sum(is.na(fossils$paleolat))
sum(is.na(fossils$paleolng))
```

While all occurrences have modern day coordinates, 18 are missing palaeocoordinates. We will now remove these from the dataset.

```{r remove_geo_NAs}
# Remove occurrences which are missing palaeocoordinates
fossils <- filter(fossils, !is.na(fossils$paleolng))
# Check whether this has worked
sum(is.na(fossils$paleolat))
sum(is.na(fossils$paleolng))
```

A further option applicable in some cases would be to fill in our `NA`s. We may be able to interpolate values from the rest of our data, or use additional data sources. For our palaeogeography example above, we could generate our own coordinates, for example using `palaeoverse::palaeorotate()`.

Let's revisit our country example from Rule 5. Does our list of country codes contain NAs?

```{r countries_3}
# Count the number of country codes for which `is.na()` is TRUE
sum(is.na(fossils$cc...36))
```

Our test tells us that two of the values are `NA`: when we asked R to sort the values, it removed `NA` from the list of unique countries. However, the PBDB has set the country within which the collection is located as a compulsory entry field. How can we have missing data here? We don't: these values are not `NA`, they are "NA" meaning Namibia, and have been misconstrued by R. This is an important illustration of why we should conduct further investigation should any apparent errors arise in the dataset.

## Rule 6: Identify outliers

[Bethany]

Next we want to focus in on the specific variables which relate to our scientific question, i.e. the geography of our fossil occurrences. First we'll plot where the crocodile fossils have been found across the globe: how does this match what we already know from the country codes?

```{r map}
# Load in a world map
world <- ne_countries(scale = "medium", returnclass = "sf")
# Plot the geographic coordinates of each locality over the world map
ggplot(fossils) +
  geom_sf(data = world) +
  geom_point(aes(x = lng, y = lat)) +
  labs(x = "Longitude",
       y = "Latitude") +
  coord_sf()
```

We have a large density of crocodile occurrences in Europe and the western interior of the United States, along with a smattering of occurrences across the other continents. This distribution seems to fit our previous knowledge, that the occurrences are spread across 46 countries. However, the crocodile occurrences in Antarctica seem particularly suspicious: crocodiles need a warm climate, and modern-day Antarctica certaintly doesn't fit this description. Let's investigate further. We'll do this by plotting the latitude of the occurrences through time.

```{r lat_time}
# Add a column to the data frame with the midpoint of the fossil ages
fossils <- fossils %>%
  mutate(mid_age = (min_ma + max_ma) / 2)
# Plot the age of each occurrence against its latitude
ggplot(fossils, aes(x = mid_age, y = lat)) +
  geom_point() +
  labs(x = "Age (Ma)",
       y = "Latitude") +
  scale_x_reverse() +
  geom_hline(yintercept = 0) +
  coord_geo(dat = "stages", expand = TRUE)
```

When we look at latitude through time, these occurrences look even more suspicious.
# Can we make the occurrences for Antarctica a different colour compared to the rest?

But, wait, we should actually be looking at **paleo**latitude instead. Let's plot that against time.

```{r paleolat_time}
# Plot the age of each occurrence against its paleolatitude
ggplot(fossils, aes(x = mid_age, y = paleolat)) +
  geom_point() +
  labs(x = "Age (Ma)",
       y = "Paleolatitude") +
  scale_x_reverse() +
  geom_hline(yintercept = 0) +
  coord_geo(dat = "stages", expand = TRUE)
```

Hmm... when we look at paleolatitude the occurrences are even **further** south. Time to really check out these occurrences. Which collections are they within?

```{r antarctic_colls}
# Find Antarctic collection numbers
Antarctic <- filter(fossils, cc...36 == "AA")
unique(Antarctic$collection_no)
```

Well, upon further visual inspection using the PBDB website, all appear to be fairly legitimate. However, all three occurrences still appear to be outliers, especially as in the late Eocene [temperatures were dropping](https://doi.org/10.1038/s41586-018-0272-2). What about the taxonomic certainty of these occurrences?

```{r antarctic_IDs}
# List taxonomic names associated with Antarctic occurrences
Antarctic$identified_name
```

Since all three occurrences are listed as "Crocodylia indet.", it may make sense to remove them from further analyses anyway.

Let's investigate if there are any other anomalies or outliers in our data. We'll bin the occurrences by stage to look for stage-level outliers.

```{r lat_time_binned}
# Put occurrences into stage bins
bins <- time_bins(scale = "international ages")
fossils <- bin_time(occdf = fossils, bins = bins,
                    min_ma = "min_ma", max_ma = "max_ma", method = "majority") %>%
                    left_join(bins, by = c("bin_assignment" = "bin"))
# Plot occurrences
ggplot(fossils, aes(x = mid_ma, y = paleolat, fill = interval_name)) +
  geom_boxplot(scale = "width", show.legend = FALSE) +
  labs(x = "Age (Ma)",
       y = "Paleolatitude") +
  scale_x_reverse() +
  scale_fill_geo("stages") +
  coord_geo(dat = "stages", expand = TRUE)
```

Box plots are a great way to look for outliers, because their calculation automatically includes outlier determination, and any such points can clearly be seen in a graph. Here, the Ypresian is looking pretty suspicious - it seems to have a lot of outliers. Let's plot the Ypresian occurrences on a paleogeographic map to investigate further.

```{r map_ypresian}
# Load map of the Ypresian, and identify Ypresian fossils
fossils_y <- fossils %>%
  filter(interval_name == "Ypresian")
world_y <- reconstruct("coastlines", age = 51.9)
# Plot localities on the Ypresian map
ggplot(fossils_y) +
  geom_sf(data = world_y) +
  geom_point(aes(x = paleolng, y = paleolat)) +
  labs(x = "Paleolongitude",
       y = "Paleolatitude") +
  coord_sf()
```

Aha! There is a concentrated cluster of occurrences in the western interior of North America. This high number of occurrences is increasing the weight of data at this palaeolatitude, and narrowing the boundaries at which other points are considered outliers. Without this strong geographic bias, all of the occurrences in the Ypresian appear to be normal, especially with [elevated temperatures during this time](https://doi.org/10.1038/s41586-018-0272-2).

```{r lat_ypresian}
# Remove US fossils from the Ypresian dataset
fossils_y <- fossils_y %>%
  filter(cc...36 != "US")
# Plot histogram of Ypresian fossil palaeolatitudes
ggplot(fossils_y) +
  geom_histogram(aes(x = paleolat)) +
  labs(x = "Paleolatitude")
```

So to sum up, it seems that our outliers are not concerning, so we will leave them in our dataset and continue with our analytical pipeline.

## Rule 7: Identify inconsistencies

We're now going to look for inconsistencies in our dataset. Let's start by revisiting its structure, focusing on whether the class types of the variables make sense.
```{r}
str(fossils)
```

Looks reasonable. Now let's dive in further to look for inconsistencies in spelling, which could cause taxonomic or geological units to be grouped separately when they are really the same thing. We'll start by checking for potential taxonomic misspellings.

We can use the table() function to look at the frequencies of various families and genera in the dataset. Here, inconsistencies like misspellings or antiquated taxonomic names might be recognized.
```{r}
#Tabulating the frequency of values in the "family" and "genus" columns
table(fossils$family)
table(fossils$genus)

#We can do the same for the specific identifications. There's some variation to check here, depending on the desired taxonomic resolution of our analysis.
table(fossils$identified_name)
table(fossils$accepted_name)
```

Alternatively, we can use the tax_check() function in the `palaeoverse` package, which systematically searches for and flags potential spelling variation using a defined dissimilarity threshold.
```{r}
#Let's see if there are any close spellings in the "genus" column
tax_spell_genus <- tax_check(taxdf=fossils,name="genus",dis=0.1)
tax_spell_genus

#We'll do the same for "accepted name" column
tax_spell_name <- tax_check(taxdf=fossils,name="accepted_name",dis=0.1)
tax_spell_name #Here, some similar names are flagged but appear to be separate but similarly spelled taxonomic groups.
```

We can also check the formatting and spelling using the `fossilbrush` package. As above, no major inconsistencies or potential spelling errors were flagged.
```{r}
#Creating a list of taxonomic ranks to check
fossil_ranks <- c("phylum","class","order","family","genus")

#Making the dataframe to check
fossils_df <- as.data.frame(fossils)

#Running the checks
tax_spell_fb <- check_taxonomy(fossils_df, ranks = fossil_ranks)
tax_spell_fb   #looking closer, the rows with "NO_FAMILY_SPECIFIED" are flagged given the extra characters
```

Let's also check the formation names while we're at it. Here, we see some potential spelling variations that will need to be corrected if we wish to use the formations in our analysis.
```{r}
#Tabulating the frequency of each formation
table(fossils$formation)

#Checking for spelling variation...note some potential misspellings here!
formation_spell <- tax_check(taxdf=fossils,name="formation",dis=0.1)
formation_spell
```

We can also look for inconsistencies in the interval names and dates (e.g., if an interval is spelled two different ways or has multiple associated dates).
```{r}
#Again, we can use the table() function to tabulate the frequency of each interval
table(fossils$early_interval)
table(fossils$late_interval)

#What are the ages associated with each interval? Any issues with the date formats?
fossils %>% 
  group_by(early_interval) %>% #grouping by interval
  distinct(max_ma.x) %>%  #pulling out the distinct values
  arrange(early_interval) #putting the list in order so it's easier to scan for inconsistencies

fossils %>% 
  group_by(late_interval) %>%
  distinct(min_ma.x) %>% 
  arrange(late_interval)
```

Finally, let's check the coordinates using the `CoordinateCleaner` package. Looks good.
```{r}
#Is the coordinate formatting OK? Numeric and part of a lat/long system?
coord_check <- cc_val(fossils, lat = "lat", lon = "lng")
paleocoord_check <- cc_val(fossils, lat = "paleolat", lon = "paleolng")

#Testing for coordinates with equal lat, long
coord_check <- cc_equ(coord_check, lat = "lat", lon = "lng")
paleocoord_check <- cc_val(paleocoord_check, lat = "paleolat", lon = "paleolng")

#Near any centroids of modern-day political units or biodiversity centers?
coord_cent <- cc_cen(coord_check, lat = "lat", lon = "lng", value = "flagged")
coord_cent <- cc_inst(coord_check, lat = "lat", lon = "lng", value = "flagged")

#any 0s?
coord_check <- cc_zero(coord_check, lat = "lat", lon = "lng")

#We can also test for temporal mismatches
coord_check <- coord_check[!is.na(coord_check$min_ma.x),]
coord_check <- coord_check[!is.na(coord_check$max_ma.x),]
coord_check <- cf_equal(coord_check, min_age = "min_ma.x", max_age = "max_ma.x")
```

Overall, our dataset looks to be in good shape!

## Rule 8: Identify duplicates [ALE]

```{r}
# Select relevant columns
fossils_geo <- fossils %>% select(accepted_name, identified_name, family, lng, lat)

# Remove duplicates based on geographic coordinates (in some spatial analyses it may be required to retain a single occurrence for spatial collection)
fossils_distinct <- fossils_geo %>% distinct(lng, lat, .keep_all = TRUE)

# Identify and flag taxonomic duplicates, in case we want to estimates diversity metrics for example, a single taxon per locality/collection may be enough
fossils_distinct <- fossils_distinct %>% 
  group_by(accepted_name) %>% 
  mutate(duplicate_flag = n() > 1)

table(fossils_distinct$duplicate_flag)

# Remove taxonomic duplicates (if flagged - so to keep record of what is duplicate in the dataset and you may want to remove later (see point 9 below)
cleaned_occurrences <- fossils_distinct %>% filter(!duplicate_flag)

# Save cleaned dataset
write.csv(cleaned_occurrences, "./all_species_occurrences_CLEANED.csv", row.names = FALSE)

# Convert to spatial data to plot the occurrences on a map
world <- ne_countries(scale = "medium", returnclass = "sf")

# Plot occurrences on the world map of different taxonomic entities included in your dataset (using pbdb accepted_name as argument)
ggplot(data = world) +
  geom_sf(fill = "gray90", color = "black") +
  geom_point(data = cleaned_occurrences, aes(x = lng, y = lat),
             alpha = 0.7, size = 2) +
  theme_minimal() +
  labs(title = "Crocodylomorpha Occurrences", x = "Longitude", y = "Latitude") +
  theme(legend.position = "bottom")
```

## Rule 9: Report your data cleaning efforts

[All]

"Here is what we would write in the paper"

## Rule 10: Deposit your data workflow and recording

[Will]

# Summary
